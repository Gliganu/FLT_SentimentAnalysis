{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.corpus import movie_reviews\n",
    "import collections\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "from nltk.classify import DecisionTreeClassifier\n",
    "from nltk.classify import MaxentClassifier\n",
    "from nltk.classify import scikitlearn\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC,LinearSVC,NuSVC\n",
    "from nltk import metrics\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "import itertools\n",
    "from nltk.classify import ClassifierI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bag_of_words(words):\n",
    "\n",
    "    return dict([(word,True) for word in words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bag_of_words_not_in_set(words,badwords):\n",
    "\n",
    "    return bag_of_words(set(words) - set(badwords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bag_of_non_stopwords(words, stopfile='english'):\n",
    "\n",
    "    badwords = stopwords.words(stopfile)\n",
    "\n",
    "    return bag_of_words_not_in_set(words,badwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bag_of_bigrams_words(words, score_fn = BigramAssocMeasures.chi_sq, n =200):\n",
    "\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "\n",
    "    bigrams = bigram_finder.nbest(score_fn,n)\n",
    "\n",
    "    return bag_of_words(words+bigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label_feats_from_corpus() function assumes that the corpus is categorized,\n",
    "and that a single file represents a single instance for feature extraction. It iterates over\n",
    "each category label, and extracts features from each file in that category using the\n",
    "feature_detector() function, which defaults to bag_of_words(). It returns a dict\n",
    "whose keys are the category labels, and the values are lists of instances for that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_feats_from_corpus(corp, feature_detector = bag_of_words):\n",
    "\n",
    "    label_feats = collections.defaultdict(list)\n",
    "\n",
    "    for label in corp.categories():\n",
    "\n",
    "        for fileid in corp.fileids(categories=[label]):\n",
    "\n",
    "            feats = feature_detector(corp.words(fileids = [fileid]))\n",
    "            label_feats[label].append(feats)\n",
    "\n",
    "    return label_feats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to split the labeled feature sets into training and testing instances using\n",
    "split_label_feats(). This function allows us to take a fair sample of labeled feature\n",
    "sets from each label, using the split keyword argument to determine the size of the sample.\n",
    "The split argument defaults to 0.75, which means the first 75% of the labeled feature sets\n",
    "for each label will be used for training, and the remaining 25% will be used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_label_feats(lfeats,split=0.75):\n",
    "\n",
    "    train_feats = []\n",
    "    test_feats = []\n",
    "\n",
    "    for label, feats in lfeats.items():\n",
    "\n",
    "        cutoff = int(len(feats)*split)\n",
    "\n",
    "        train_feats.extend([(feat,label) for feat in feats[:cutoff]])\n",
    "        test_feats.extend([(feat, label) for feat in feats[cutoff:]])\n",
    "\n",
    "    return train_feats,test_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pos', 'neg'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfeats = label_feats_from_corpus(movie_reviews)\n",
    "\n",
    "lfeats.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_feats, test_feats = split_label_feats(lfeats,split=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(train_feats))\n",
    "print(len(test_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_classifier = NaiveBayesClassifier.train(train_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pos', 'neg']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classifier.labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n"
     ]
    }
   ],
   "source": [
    "review = bag_of_words(['the','plot','was','accessible'])\n",
    "print(nb_classifier.classify(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the accuracy of the classifier using nltk.classify.util.accuracy()\n",
    "and the test_feats variable created previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.728"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(nb_classifier,test_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the classify() method returns only a single label, you can use the\n",
    "prob_classify() method to get the classification probability of each label.\n",
    "This can be useful if you want to use probability thresholds for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pos', 'neg'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = nb_classifier.prob_classify(test_feats[0][0])\n",
    "probs.samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.744195869104262e-21\n"
     ]
    }
   ],
   "source": [
    "print(probs.prob('pos'))\n",
    "print(probs.prob('neg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The show_most_informative_features() method will print out the results from\n",
    "most_informative_features() and will also include the probability of a feature\n",
    "pair belonging to each label:\n",
    "\n",
    "The informativeness, or information gain, of each feature pair is based on the prior\n",
    "probability of the feature pair occurring for each label.More informative features are\n",
    "those that occur primarily in one label and not on the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             magnificent = True              pos : neg    =     15.0 : 1.0\n",
      "             outstanding = True              pos : neg    =     13.6 : 1.0\n",
      "               insulting = True              neg : pos    =     13.0 : 1.0\n",
      "              vulnerable = True              pos : neg    =     12.3 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "nb_classifier.show_most_informative_features(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using DecisionTreeClassifier\n",
    "\n",
    "The entropy_cutoff value is used during the tree refinement process. The tree refinement\n",
    "process is how the decision tree decides to create new branches. If the entropy of the\n",
    "probability distribution of label choices in the tree is greater than the entropy_cutoff\n",
    "value, then the tree is refined further by creating more branches. But if the entropy is lower\n",
    "than the entropy_cutoff value, then tree refinement is halted.\n",
    "\n",
    "The depth_cutoff value is also used during refinement to control the depth of the tree.\n",
    "The final decision tree will never be deeper than the depth_cutoff value. The default\n",
    "value is 100, which means that classification may require up to 100 decisions before\n",
    "reaching a leaf node. Decreasing the depth_cutoff value will decrease the training time\n",
    "and most likely decrease the accuracy as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-2fc3ed42fddb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdt_classifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_feats\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msupport_cutoff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\GligaBogdan\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m             tree = DecisionTreeClassifier.best_binary_stump(\n\u001b[1;32m--> 157\u001b[1;33m                 feature_names, labeled_featuresets, feature_values, verbose)\n\u001b[0m\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\GligaBogdan\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mbest_binary_stump\u001b[1;34m(feature_names, labeled_featuresets, feature_values, verbose)\u001b[0m\n\u001b[0;32m    262\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeature_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m                 stump = DecisionTreeClassifier.binary_stump(\n\u001b[1;32m--> 264\u001b[1;33m                     fname, fval, labeled_featuresets)\n\u001b[0m\u001b[0;32m    265\u001b[0m                 \u001b[0mstump_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstump\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabeled_featuresets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mstump_error\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mbest_error\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\GligaBogdan\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mbinary_stump\u001b[1;34m(feature_name, feature_value, labeled_featuresets)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbinary_stump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabeled_featuresets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         label = FreqDist(label for (featureset, label)\n\u001b[1;32m--> 234\u001b[1;33m                          in labeled_featuresets).max()\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;31m# Find the best label for each value.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\GligaBogdan\\Anaconda3\\lib\\site-packages\\nltk\\probability.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, samples)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \"\"\"\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[0mCounter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\GligaBogdan\\Anaconda3\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    512\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'expected at most 1 arguments, got %d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 514\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\GligaBogdan\\Anaconda3\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    599\u001b[0m                     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# fast path when counter is empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m                 \u001b[0m_count_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\GligaBogdan\\Anaconda3\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbinary_stump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabeled_featuresets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m         label = FreqDist(label for (featureset, label)\n\u001b[0m\u001b[0;32m    234\u001b[0m                          in labeled_featuresets).max()\n\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dt_classifier = DecisionTreeClassifier.train(train_feats,binary=True, entropy_cutoff=0.8, depth_cutoff=5, support_cutoff=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy(dt_classifier,test_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a maximum entropy classifier\n",
    "\n",
    "The third classifier we will cover is the MaxentClassifier class, also known as a\n",
    "conditional exponential classifier or logistic regression classifier. The maximum\n",
    "entropy classifier converts labeled feature sets to vectors using encoding. This encoded\n",
    "vector is then used to calculate weights for each feature that can then be combined to\n",
    "determine the most likely label for a feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "me_classifier = MaxentClassifier.train(train_feats, algorithm='gis', trace=0, max_iter=10, min_lldelta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy(me_classifier, test_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training scikit-learn classifiers Scikit-learn\n",
    "\n",
    "We won't be accessing the scikit-learn models directly in this recipe. Instead,\n",
    "we'll be using NLTK's SklearnClassifier class, which is a wrapper class around a\n",
    "scikit-learn model to make it conform to NLTK's ClassifierI interface\n",
    "\n",
    "Training an SklearnClassifier class has a slightly different series of steps than classifiers\n",
    "covered in the previous recipes of this chapter:\n",
    "1. Create training features (covered in the previous recipes).\n",
    "2. Choose and import an sklearn algorithm.\n",
    "3. Construct an SklearnClassifier class with the chosen algorithm.\n",
    "4. Train the SklearnClassifier class with your training features.\n",
    "\n",
    "\n",
    "\n",
    "The SklearnClassifier class is a small wrapper class whose main job is to convert NLTK\n",
    "feature dictionaries into sklearn compatible feature vectors\n",
    "\n",
    "But not all the classification algorithms are compatible with the\n",
    "SklearnClassifier class, because it uses sparse vectors. Sparse vectors are more\n",
    "efficient because they only store the data they need, using a kind of data compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_classifier = SklearnClassifier(MultinomialNB())\n",
    "sk_classifier.train(train_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(sk_classifier, test_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try it with BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True))>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_classifier = SklearnClassifier(BernoulliNB())\n",
    "sk_classifier.train(train_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.812"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(sk_classifier, test_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sk_classifier = SklearnClassifier(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_classifier = SklearnClassifier(LogisticRegression())\n",
    "sk_classifier.train(train_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.892"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(sk_classifier, test_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see that the sklearn algorithm has better performance than NLTK's\n",
    "MaxentClassifier, which only had 72.2% accuracy. The logistic regression algorithm\n",
    "also has a much faster training time than the IIS or GIS algorithms, even when those\n",
    "algorithms have a limited number of iterations. This can be explained by sklearn's focus\n",
    "on optimized numeric processing using NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A third family of algorithms that NLTK does not support directly is Support Vector\n",
    "Machines, or SVM. These algorithms have been shown to be effective at learning\n",
    "on high-dimensional data, such as text classification, where every word feature\n",
    "counts as a dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_classifier = SklearnClassifier(SVC())\n",
    "sk_classifier.train(train_feats)\n",
    "accuracy(sk_classifier, test_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.864"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_classifier = SklearnClassifier(LinearSVC())\n",
    "sk_classifier.train(train_feats)\n",
    "accuracy(sk_classifier, test_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.882"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_classifier = SklearnClassifier(NuSVC())\n",
    "sk_classifier.train(train_feats)\n",
    "accuracy(sk_classifier, test_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to accuracy, there are a number of other metrics used to evaluate classifiers.\n",
    "Two of the most common are precision and recall. To understand these two metrics, we must\n",
    "first understand false positives and false negatives. False positives happen when a classifier\n",
    "classifies a feature set with a label it shouldn't have gotten. False negatives happen when a\n",
    "classifier doesn't assign a label to a feature set that should have it. In a binary classifier, these\n",
    "errors happen at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The low information words are words that are common to all labels. It may be counter-intuitive,\n",
    "but eliminating these words from the training data can actually improve accuracy, precision, and\n",
    "recall. The reason this works is that using only high information words reduces the noise and\n",
    "confusion of a classifier's internal model. If all the words/features are highly biased one way or\n",
    "the other, it's much easier for the classifier to make a correct guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bag_of_words_in_set(words, goodwords):\n",
    "    return bag_of_words(set(words) & set(goodwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high_information_words() function starts by counting the frequency of every word,\n",
    "as well as the conditional frequency for each word within each label.Once we have the FreqDist and ConditionalFreqDist variables, we can score each\n",
    "word on a per-label basis\n",
    "\n",
    "\n",
    "The default score_fn is nltk.metrics.BigramAssocMeasures.chi_sq(), which\n",
    "calculates the chi-square score for each word using the following parameters:\n",
    "1. n_ii: This is the frequency of the word for the label\n",
    "2. n_ix: This is the total frequency of the word across all labels\n",
    "3. n_xi: This is the total frequency of all words that occurred for the label\n",
    "4. n_xx: This is the total frequency for all words in all labels\n",
    "\n",
    "The simplest way to think about these numbers is that the closer n_ii is to n_ix, the higher\n",
    "the score. Or, the more often a word occurs in a label, relative to its overall occurrence, the\n",
    "higher the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def high_information_words(labelled_words, score_fn=BigramAssocMeasures.chi_sq, min_score=5):\n",
    "    word_fd = FreqDist()\n",
    "    label_word_fd = ConditionalFreqDist()\n",
    "    \n",
    "    for label,words in labelled_words:\n",
    "        \n",
    "        for word in words:\n",
    "            word_fd[word] += 1\n",
    "            label_word_fd[label][word] += 1\n",
    "            \n",
    "    n_xx = label_word_fd.N()\n",
    "    high_info_words = set()\n",
    "    \n",
    "    for label in label_word_fd.conditions():\n",
    "        \n",
    "        n_xi = label_word_fd[label].N()\n",
    "        word_scores = collections.defaultdict(int)\n",
    "        \n",
    "        for word,n_ii in label_word_fd[label].items():\n",
    "            \n",
    "            n_ix = word_fd[word]\n",
    "            score = score_fn(n_ii,(n_ix,n_xi),n_xx)\n",
    "            word_scores[word] = score\n",
    "            \n",
    "            \n",
    "        bestwords = [word for word,score in word_scores.items() if score >=min_score]\n",
    "        high_info_words |= set(bestwords)\n",
    "        \n",
    "    return high_info_words\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this new feature detector, we can call label_feats_from_corpus() and get a\n",
    "new train_feats and test_feats function using split_label_feats()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = movie_reviews.categories()\n",
    "labeled_words = [(label,movie_reviews.words(categories=[label])) for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "high_info_words = set(high_information_words(labeled_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat_det = lambda words: bag_of_words_in_set(words,high_info_words)\n",
    "lfeats = label_feats_from_corpus(movie_reviews,feature_detector=feat_det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_feats,test_feats = split_label_feats(lfeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have new training and testing feature sets, let's train and evaluate a\n",
    "NaiveBayesClassifier class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_classifier = NaiveBayesClassifier.train(train_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(nb_classifier, test_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to improve classification performance is to combine classifiers. The simplest way to\n",
    "combine multiple classifiers is to use voting, and choose whichever label gets the most votes.\n",
    "For this style of voting, it's best to have an odd number of classifiers so that there are no ties.\n",
    "This means combining at least three classifiers together. The individual classifiers should\n",
    "also use different algorithms; the idea is that multiple algorithms are better than one, and\n",
    "the combination of many can compensate for individual bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MaxVoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "        self._labels = sorted(set(itertools.chain(*[c.labels() for c in classifiers])))\n",
    "\n",
    "    def labels(self):\n",
    "        return self._labels\n",
    "    \n",
    "    def classify(self, feats):\n",
    "        counts = FreqDist()\n",
    "        for classifier in self._classifiers:\n",
    "            counts[classifier.classify(feats)] += 1\n",
    "        \n",
    "        return counts.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_classifier = NaiveBayesClassifier.train(train_feats)\n",
    "svc_classifier = SklearnClassifier(NuSVC()).train(train_feats)\n",
    "mnb_classifier = SklearnClassifier(MultinomialNB()).train(train_feats)\n",
    "\n",
    "mv_classifier = MaxVoteClassifier(nb_classifier, svc_classifier,mnb_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.918"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(mv_classifier, test_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
